{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier, AdaBoostRegressor,GradientBoostingClassifier\n",
    "import xgboost  # You may need to pip install this!\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the problems with using single decision trees and random forests is that, once I make a split, I can't go back and consider how another feature varies across the whole dataset. But suppose I were to consider **my tree's errors**. The fundamental idea of ***boosting*** is to start with a weak learner and then to use information about its errors to build a new model that can supplement the original model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two Types\n",
    "\n",
    "The two main types of boosting available in Scikit-Learn are adaptive boosting (AdaBoostClassifier, AdaBoostRegressor) and gradient boosting (GradientBoostingClassifier, GradientBoostingRegressor).\n",
    "\n",
    "Again, the fundamental idea of boosting is to use a sequence of **weak** learners to build a model. Though the individual learners are weak, the idea is to train iteratively in order to produce a better predictor. More specifically, the first learner will be trained on the data as it stands, but future learners will be trained on modified versions of the data. The point of the modifications is to highlight the \"hard-to-predict-accurately\" portions of the data.\n",
    "\n",
    "- **AdaBoost** works by iteratively adapting two related series of weights, one attached to the datapoints and the other attached to the learners themselves. Datapoints that are incorrectly classified receive greater weights for the next learner in the sequence. That way, future learners will be more likely to focus on those datapoints. At the end of the sequence, the learners that make better predictions, especially on the datapoints that are more resistant to correct classification, receive more weight in the final \"vote\" that determines the ensemble's prediction. <br/> Suppose we have binary classification problem and we represent the two classes with 1 and -1. (This is standard for describing the algorithm of AdaBoost.) <br/>\n",
    "Then, in a nutshell: <br/>\n",
    "    1. Train a weak learner. <br/>\n",
    "    2. Calculate its error $\\epsilon$. <br/>\n",
    "    3. Use that error as a weight on the classifier: $\\theta = \\frac{1}{2}ln\\left(\\frac{1-\\epsilon}{\\epsilon}\\right)$. <br/>\n",
    "    Note that $\\theta$ CAN be negative. This represents a classifier whose accuracy is _worse_ than chance. <br/>\n",
    "    4. Use _that_ to adjust the data points' weights: $w_{n+1} = w_n\\left(\\frac{e^{\\pm\\theta}}{scaler}\\right)$. Use $+\\theta$ for incorrect predictions, $-\\theta$ for correct predictions. <br/>  $\\rightarrow$ For more detail on AdaBoost, see [here](https://towardsdatascience.com/boosting-algorithm-adaboost-b6737a9ee60c).\n",
    "\n",
    "- **Gradient Boosting** works instead by training each new learner on the residuals of the model built with the learners that have so far been constructed. That is, Model $n+1$ (with $n+1$ learners) will focus on the predictions of Model $n$ (with only $n$ learners) that were **most off the mark**. As the training process repeats, the learners learn and the residuals get smaller. I would get a sequence going: <br/> Model 0 is very simple. Perhaps it merely predicts the mean: <br/>\n",
    "$\\hat{y}_0 = \\bar{y}$; <br/>\n",
    "Model 1's predictions would then be the sum of (i) Model 0's predictions and (ii) the predictions of the model fitted to Model 0's residuals: <br/> $\\hat{y}_1 = \\hat{y}_0 + \\hat{(y - \\hat{y})}_{err0}$; <br/>\n",
    "Now iterate: Model 2's predictions will be the sum of (i) Model 0's predictions, (ii) the predictions of the model fitted to Model 0's residuals, and (iii) the predictions of the model fitted to Model 1's residuals: <br/> $\\hat{y}_2 = \\hat{y}_0 + \\hat{(y - \\hat{y})}_{err0} + \\hat{(y - \\hat{y})}_{err1}$<br/>\n",
    "Etc.\n",
    "<br/>\n",
    "\n",
    "For more on this idea, see [here](http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/). <br/> $\\rightarrow$ How does gradient boosting work for a classification problem? How do we even make sense of the notion of a gradient in that context? The short answer is that we appeal to the probabilities associated with the predictions for the various classes. See more on this topic [here](https://sefiks.com/2018/10/29/a-step-by-step-gradient-boosting-example-for-classification/). <br/> $\\rightarrow$ Why is this called \"_gradient_ boosting\"? The short answer is that fitting a learner to a model's residuals comes to the same thing as fitting it to the derivative of that model's loss function. See more on this topic [here](https://www.ritchievink.com/blog/2018/11/19/algorithm-breakdown-why-do-we-call-it-gradient-boosting/).\n",
    "\n",
    "## AdaBoost in Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nr</th>\n",
       "      <th>Rmag</th>\n",
       "      <th>e.Rmag</th>\n",
       "      <th>ApDRmag</th>\n",
       "      <th>mumax</th>\n",
       "      <th>Mcz</th>\n",
       "      <th>e.Mcz</th>\n",
       "      <th>MCzml</th>\n",
       "      <th>chi2red</th>\n",
       "      <th>UjMAG</th>\n",
       "      <th>...</th>\n",
       "      <th>UFS</th>\n",
       "      <th>e.UFS</th>\n",
       "      <th>BFS</th>\n",
       "      <th>e.BFS</th>\n",
       "      <th>VFD</th>\n",
       "      <th>e.VFD</th>\n",
       "      <th>RFS</th>\n",
       "      <th>e.RFS</th>\n",
       "      <th>IFD</th>\n",
       "      <th>e.IFD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>24.995</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.935</td>\n",
       "      <td>24.214</td>\n",
       "      <td>0.832</td>\n",
       "      <td>0.036</td>\n",
       "      <td>1.400</td>\n",
       "      <td>0.64</td>\n",
       "      <td>-17.67</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01870</td>\n",
       "      <td>0.00239</td>\n",
       "      <td>0.01630</td>\n",
       "      <td>0.00129</td>\n",
       "      <td>0.017300</td>\n",
       "      <td>0.00141</td>\n",
       "      <td>0.01650</td>\n",
       "      <td>0.000434</td>\n",
       "      <td>0.02470</td>\n",
       "      <td>0.00483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>25.013</td>\n",
       "      <td>0.181</td>\n",
       "      <td>-0.135</td>\n",
       "      <td>25.303</td>\n",
       "      <td>0.927</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.864</td>\n",
       "      <td>0.41</td>\n",
       "      <td>-18.28</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00706</td>\n",
       "      <td>0.00238</td>\n",
       "      <td>0.00420</td>\n",
       "      <td>0.00115</td>\n",
       "      <td>0.003930</td>\n",
       "      <td>0.00182</td>\n",
       "      <td>0.00723</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.00973</td>\n",
       "      <td>0.00460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>24.246</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.821</td>\n",
       "      <td>23.511</td>\n",
       "      <td>1.202</td>\n",
       "      <td>0.037</td>\n",
       "      <td>1.217</td>\n",
       "      <td>0.92</td>\n",
       "      <td>-19.75</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01260</td>\n",
       "      <td>0.00184</td>\n",
       "      <td>0.01830</td>\n",
       "      <td>0.00115</td>\n",
       "      <td>0.018800</td>\n",
       "      <td>0.00167</td>\n",
       "      <td>0.02880</td>\n",
       "      <td>0.000655</td>\n",
       "      <td>0.05700</td>\n",
       "      <td>0.00465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>25.203</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.639</td>\n",
       "      <td>24.948</td>\n",
       "      <td>0.912</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.39</td>\n",
       "      <td>-17.83</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01410</td>\n",
       "      <td>0.00186</td>\n",
       "      <td>0.01180</td>\n",
       "      <td>0.00110</td>\n",
       "      <td>0.009670</td>\n",
       "      <td>0.00204</td>\n",
       "      <td>0.01050</td>\n",
       "      <td>0.000416</td>\n",
       "      <td>0.01340</td>\n",
       "      <td>0.00330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>26</td>\n",
       "      <td>25.504</td>\n",
       "      <td>0.112</td>\n",
       "      <td>-1.588</td>\n",
       "      <td>24.934</td>\n",
       "      <td>0.848</td>\n",
       "      <td>0.067</td>\n",
       "      <td>1.330</td>\n",
       "      <td>1.45</td>\n",
       "      <td>-17.69</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00514</td>\n",
       "      <td>0.00170</td>\n",
       "      <td>0.00102</td>\n",
       "      <td>0.00127</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.00160</td>\n",
       "      <td>0.00139</td>\n",
       "      <td>0.000499</td>\n",
       "      <td>0.00590</td>\n",
       "      <td>0.00444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Nr    Rmag  e.Rmag  ApDRmag   mumax    Mcz  e.Mcz  MCzml  chi2red  UjMAG  \\\n",
       "0   6  24.995   0.097    0.935  24.214  0.832  0.036  1.400     0.64 -17.67   \n",
       "1   9  25.013   0.181   -0.135  25.303  0.927  0.122  0.864     0.41 -18.28   \n",
       "2  16  24.246   0.054    0.821  23.511  1.202  0.037  1.217     0.92 -19.75   \n",
       "3  21  25.203   0.128    0.639  24.948  0.912  0.177  0.776     0.39 -17.83   \n",
       "4  26  25.504   0.112   -1.588  24.934  0.848  0.067  1.330     1.45 -17.69   \n",
       "\n",
       "   ...      UFS    e.UFS      BFS    e.BFS       VFD    e.VFD      RFS  \\\n",
       "0  ...  0.01870  0.00239  0.01630  0.00129  0.017300  0.00141  0.01650   \n",
       "1  ...  0.00706  0.00238  0.00420  0.00115  0.003930  0.00182  0.00723   \n",
       "2  ...  0.01260  0.00184  0.01830  0.00115  0.018800  0.00167  0.02880   \n",
       "3  ...  0.01410  0.00186  0.01180  0.00110  0.009670  0.00204  0.01050   \n",
       "4  ...  0.00514  0.00170  0.00102  0.00127  0.000039  0.00160  0.00139   \n",
       "\n",
       "      e.RFS      IFD    e.IFD  \n",
       "0  0.000434  0.02470  0.00483  \n",
       "1  0.000500  0.00973  0.00460  \n",
       "2  0.000655  0.05700  0.00465  \n",
       "3  0.000416  0.01340  0.00330  \n",
       "4  0.000499  0.00590  0.00444  \n",
       "\n",
       "[5 rows x 65 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "galaxies = pd.read_csv('COMBO17.csv')\n",
    "galaxies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a dataset about galaxies. The Mcz and MCzml columns are measures of redshift, which is our target. Mcz is usually understood to be a better measure, so that will be our target column. Many of the other columns have to do with various measures of galaxies' magnitudes. For more on the dataset, see [here](https://astrostatistics.psu.edu/datasets/COMBO17.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Nr', 'Rmag', 'e.Rmag', 'ApDRmag', 'mumax', 'Mcz', 'e.Mcz', 'MCzml',\n",
       "       'chi2red', 'UjMAG', 'e.UjMAG', 'BjMAG', 'e.BjMAG', 'VjMAG', 'e.VjMAG',\n",
       "       'usMAG', 'e.usMAG', 'gsMAG', 'e.gsMAG', 'rsMAG', 'e.rsMAG', 'UbMAG',\n",
       "       'e.UbMAG', 'BbMAG', 'e.BbMAG', 'VnMAG', 'e.VbMAG', 'S280MAG',\n",
       "       'e.S280MA', 'W420FE', 'e.W420FE', 'W462FE', 'e.W462FE', 'W485FD',\n",
       "       'e.W485FD', 'W518FE', 'e.W518FE', 'W571FS', 'e.W571FS', 'W604FE',\n",
       "       'e.W604FE', 'W646FD', 'e.W646FD', 'W696FE', 'e.W696FE', 'W753FE',\n",
       "       'e.W753FE', 'W815FS', 'e.W815FS', 'W856FD', 'e.W856FD', 'W914FD',\n",
       "       'e.W914FD', 'W914FE', 'e.W914FE', 'UFS', 'e.UFS', 'BFS', 'e.BFS', 'VFD',\n",
       "       'e.VFD', 'RFS', 'e.RFS', 'IFD', 'e.IFD'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "galaxies.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "galaxies.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "galaxies = galaxies.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's collect together the columns that have high correlation with Mcz, our target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "for ind in galaxies.corr()['Mcz'].index:\n",
    "    if abs(galaxies.corr()['Mcz'][ind]) > 0.5:\n",
    "        preds.append(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mcz</th>\n",
       "      <th>e.Mcz</th>\n",
       "      <th>MCzml</th>\n",
       "      <th>UjMAG</th>\n",
       "      <th>BjMAG</th>\n",
       "      <th>VjMAG</th>\n",
       "      <th>usMAG</th>\n",
       "      <th>gsMAG</th>\n",
       "      <th>rsMAG</th>\n",
       "      <th>UbMAG</th>\n",
       "      <th>BbMAG</th>\n",
       "      <th>VnMAG</th>\n",
       "      <th>S280MAG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Mcz</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.613071</td>\n",
       "      <td>0.872016</td>\n",
       "      <td>-0.699550</td>\n",
       "      <td>-0.628226</td>\n",
       "      <td>-0.641983</td>\n",
       "      <td>-0.698304</td>\n",
       "      <td>-0.652170</td>\n",
       "      <td>-0.631398</td>\n",
       "      <td>-0.699731</td>\n",
       "      <td>-0.659506</td>\n",
       "      <td>-0.641870</td>\n",
       "      <td>-0.700453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>e.Mcz</td>\n",
       "      <td>0.613071</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.590179</td>\n",
       "      <td>-0.170296</td>\n",
       "      <td>-0.122309</td>\n",
       "      <td>-0.122616</td>\n",
       "      <td>-0.168326</td>\n",
       "      <td>-0.126491</td>\n",
       "      <td>-0.114022</td>\n",
       "      <td>-0.170208</td>\n",
       "      <td>-0.130680</td>\n",
       "      <td>-0.122546</td>\n",
       "      <td>-0.169344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>MCzml</td>\n",
       "      <td>0.872016</td>\n",
       "      <td>0.590179</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.565663</td>\n",
       "      <td>-0.520006</td>\n",
       "      <td>-0.532880</td>\n",
       "      <td>-0.563591</td>\n",
       "      <td>-0.536750</td>\n",
       "      <td>-0.525495</td>\n",
       "      <td>-0.565406</td>\n",
       "      <td>-0.544325</td>\n",
       "      <td>-0.532837</td>\n",
       "      <td>-0.562474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>UjMAG</td>\n",
       "      <td>-0.699550</td>\n",
       "      <td>-0.170296</td>\n",
       "      <td>-0.565663</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.933252</td>\n",
       "      <td>0.961088</td>\n",
       "      <td>0.999865</td>\n",
       "      <td>0.970124</td>\n",
       "      <td>0.954853</td>\n",
       "      <td>0.999986</td>\n",
       "      <td>0.975476</td>\n",
       "      <td>0.960981</td>\n",
       "      <td>0.963415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>BjMAG</td>\n",
       "      <td>-0.628226</td>\n",
       "      <td>-0.122309</td>\n",
       "      <td>-0.520006</td>\n",
       "      <td>0.933252</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.950728</td>\n",
       "      <td>0.932328</td>\n",
       "      <td>0.954006</td>\n",
       "      <td>0.947597</td>\n",
       "      <td>0.932831</td>\n",
       "      <td>0.957836</td>\n",
       "      <td>0.950694</td>\n",
       "      <td>0.885382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>VjMAG</td>\n",
       "      <td>-0.641983</td>\n",
       "      <td>-0.122616</td>\n",
       "      <td>-0.532880</td>\n",
       "      <td>0.961088</td>\n",
       "      <td>0.950728</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.959432</td>\n",
       "      <td>0.997956</td>\n",
       "      <td>0.999324</td>\n",
       "      <td>0.960265</td>\n",
       "      <td>0.992178</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.894171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>usMAG</td>\n",
       "      <td>-0.698304</td>\n",
       "      <td>-0.168326</td>\n",
       "      <td>-0.563591</td>\n",
       "      <td>0.999865</td>\n",
       "      <td>0.932328</td>\n",
       "      <td>0.959432</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.968971</td>\n",
       "      <td>0.953027</td>\n",
       "      <td>0.999903</td>\n",
       "      <td>0.974515</td>\n",
       "      <td>0.959320</td>\n",
       "      <td>0.964972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>gsMAG</td>\n",
       "      <td>-0.652170</td>\n",
       "      <td>-0.126491</td>\n",
       "      <td>-0.536750</td>\n",
       "      <td>0.970124</td>\n",
       "      <td>0.954006</td>\n",
       "      <td>0.997956</td>\n",
       "      <td>0.968971</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995436</td>\n",
       "      <td>0.969562</td>\n",
       "      <td>0.995887</td>\n",
       "      <td>0.997923</td>\n",
       "      <td>0.914118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>rsMAG</td>\n",
       "      <td>-0.631398</td>\n",
       "      <td>-0.114022</td>\n",
       "      <td>-0.525495</td>\n",
       "      <td>0.954853</td>\n",
       "      <td>0.947597</td>\n",
       "      <td>0.999324</td>\n",
       "      <td>0.953027</td>\n",
       "      <td>0.995436</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.953909</td>\n",
       "      <td>0.988695</td>\n",
       "      <td>0.999341</td>\n",
       "      <td>0.882782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>UbMAG</td>\n",
       "      <td>-0.699731</td>\n",
       "      <td>-0.170208</td>\n",
       "      <td>-0.565406</td>\n",
       "      <td>0.999986</td>\n",
       "      <td>0.932831</td>\n",
       "      <td>0.960265</td>\n",
       "      <td>0.999903</td>\n",
       "      <td>0.969562</td>\n",
       "      <td>0.953909</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.975035</td>\n",
       "      <td>0.960155</td>\n",
       "      <td>0.964189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>BbMAG</td>\n",
       "      <td>-0.659506</td>\n",
       "      <td>-0.130680</td>\n",
       "      <td>-0.544325</td>\n",
       "      <td>0.975476</td>\n",
       "      <td>0.957836</td>\n",
       "      <td>0.992178</td>\n",
       "      <td>0.974515</td>\n",
       "      <td>0.995887</td>\n",
       "      <td>0.988695</td>\n",
       "      <td>0.975035</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.992127</td>\n",
       "      <td>0.925047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>VnMAG</td>\n",
       "      <td>-0.641870</td>\n",
       "      <td>-0.122546</td>\n",
       "      <td>-0.532837</td>\n",
       "      <td>0.960981</td>\n",
       "      <td>0.950694</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.959320</td>\n",
       "      <td>0.997923</td>\n",
       "      <td>0.999341</td>\n",
       "      <td>0.960155</td>\n",
       "      <td>0.992127</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.893972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>S280MAG</td>\n",
       "      <td>-0.700453</td>\n",
       "      <td>-0.169344</td>\n",
       "      <td>-0.562474</td>\n",
       "      <td>0.963415</td>\n",
       "      <td>0.885382</td>\n",
       "      <td>0.894171</td>\n",
       "      <td>0.964972</td>\n",
       "      <td>0.914118</td>\n",
       "      <td>0.882782</td>\n",
       "      <td>0.964189</td>\n",
       "      <td>0.925047</td>\n",
       "      <td>0.893972</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Mcz     e.Mcz     MCzml     UjMAG     BjMAG     VjMAG     usMAG  \\\n",
       "Mcz      1.000000  0.613071  0.872016 -0.699550 -0.628226 -0.641983 -0.698304   \n",
       "e.Mcz    0.613071  1.000000  0.590179 -0.170296 -0.122309 -0.122616 -0.168326   \n",
       "MCzml    0.872016  0.590179  1.000000 -0.565663 -0.520006 -0.532880 -0.563591   \n",
       "UjMAG   -0.699550 -0.170296 -0.565663  1.000000  0.933252  0.961088  0.999865   \n",
       "BjMAG   -0.628226 -0.122309 -0.520006  0.933252  1.000000  0.950728  0.932328   \n",
       "VjMAG   -0.641983 -0.122616 -0.532880  0.961088  0.950728  1.000000  0.959432   \n",
       "usMAG   -0.698304 -0.168326 -0.563591  0.999865  0.932328  0.959432  1.000000   \n",
       "gsMAG   -0.652170 -0.126491 -0.536750  0.970124  0.954006  0.997956  0.968971   \n",
       "rsMAG   -0.631398 -0.114022 -0.525495  0.954853  0.947597  0.999324  0.953027   \n",
       "UbMAG   -0.699731 -0.170208 -0.565406  0.999986  0.932831  0.960265  0.999903   \n",
       "BbMAG   -0.659506 -0.130680 -0.544325  0.975476  0.957836  0.992178  0.974515   \n",
       "VnMAG   -0.641870 -0.122546 -0.532837  0.960981  0.950694  0.999997  0.959320   \n",
       "S280MAG -0.700453 -0.169344 -0.562474  0.963415  0.885382  0.894171  0.964972   \n",
       "\n",
       "            gsMAG     rsMAG     UbMAG     BbMAG     VnMAG   S280MAG  \n",
       "Mcz     -0.652170 -0.631398 -0.699731 -0.659506 -0.641870 -0.700453  \n",
       "e.Mcz   -0.126491 -0.114022 -0.170208 -0.130680 -0.122546 -0.169344  \n",
       "MCzml   -0.536750 -0.525495 -0.565406 -0.544325 -0.532837 -0.562474  \n",
       "UjMAG    0.970124  0.954853  0.999986  0.975476  0.960981  0.963415  \n",
       "BjMAG    0.954006  0.947597  0.932831  0.957836  0.950694  0.885382  \n",
       "VjMAG    0.997956  0.999324  0.960265  0.992178  0.999997  0.894171  \n",
       "usMAG    0.968971  0.953027  0.999903  0.974515  0.959320  0.964972  \n",
       "gsMAG    1.000000  0.995436  0.969562  0.995887  0.997923  0.914118  \n",
       "rsMAG    0.995436  1.000000  0.953909  0.988695  0.999341  0.882782  \n",
       "UbMAG    0.969562  0.953909  1.000000  0.975035  0.960155  0.964189  \n",
       "BbMAG    0.995887  0.988695  0.975035  1.000000  0.992127  0.925047  \n",
       "VnMAG    0.997923  0.999341  0.960155  0.992127  1.000000  0.893972  \n",
       "S280MAG  0.914118  0.882782  0.964189  0.925047  0.893972  1.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "galaxies[preds].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These various magnitude columns all have high correlations **with one another**! Let's try a simple model with just the S280MAG column, since it has the highest correlation with Mcz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = galaxies['S280MAG']\n",
    "y = galaxies['Mcz']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we only have one predictor, we can visualize the correlation with the target! We can also reshape it for modeling purposes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_rev = x.values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mpl.pyplot.scatter(x_rev, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_rev, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostRegressor(base_estimator=None, learning_rate=1.0, loss='linear',\n",
       "                  n_estimators=50, random_state=42)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abr = AdaBoostRegressor(random_state=42)\n",
    "\n",
    "abr.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.50416011, 0.51277109, 0.58225908, 0.47066606, 0.58004547])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(abr, x_test, y_test, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "Let's see if we can do better by trying different hyperparameter values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GridSearchCV(estimator=abr,\n",
    "                 param_grid={\n",
    "                     'n_estimators': [25, 50, 100],\n",
    "                     'loss': ['linear', 'square']\n",
    "                 }, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=AdaBoostRegressor(base_estimator=None, learning_rate=1.0,\n",
       "                                         loss='linear', n_estimators=50,\n",
       "                                         random_state=42),\n",
       "             iid='warn', n_jobs=None,\n",
       "             param_grid={'loss': ['linear', 'square'],\n",
       "                         'n_estimators': [25, 50, 100]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': 'linear', 'n_estimators': 25}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
       "             max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "             n_jobs=1, nthread=None, objective='reg:squarederror',\n",
       "             random_state=42, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "             seed=None, silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_boost = xgboost.XGBRegressor(random_state=42, objective='reg:squarederror')\n",
    "\n",
    "grad_boost.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.49371958, 0.52220505, 0.5979291 , 0.49283617, 0.65196486])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(grad_boost, x_test, y_test, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression or Classification?\n",
    "\n",
    "What does my target look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "galaxies['Mcz'].hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be a bit of a bimodal shape here. We might therefore try predicting whether the redshift factor is likely to be greater or less than 0.5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "galaxies['bool'] = galaxies['Mcz'] > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train2, x_test2, y_train2, y_test2 = train_test_split(x_rev, galaxies['bool'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                   n_estimators=50, random_state=42)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abc = AdaBoostClassifier(random_state=42)\n",
    "\n",
    "abc.fit(x_train2, y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8860465116279069"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abc.score(x_test2, y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8958944281524927"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_test2, abc.predict(x_test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.957680250783699"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_test2, abc.predict(x_test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "                           max_features=None, max_leaf_nodes=None,\n",
       "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                           min_samples_leaf=1, min_samples_split=2,\n",
       "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                           n_iter_no_change=None, presort='auto',\n",
       "                           random_state=42, subsample=1.0, tol=0.0001,\n",
       "                           validation_fraction=0.1, verbose=0,\n",
       "                           warm_start=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbc = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "gbc.fit(x_test2, y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.922093023255814"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbc.score(x_test2, y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.911976911976912"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_test2, gbc.predict(x_test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9905956112852664"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_test2, gbc.predict(x_test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[161,  61],\n",
       "       [  6, 632]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test2, gbc.predict(x_test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
